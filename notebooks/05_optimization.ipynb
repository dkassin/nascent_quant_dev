{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "### Introduction\n",
    "\n",
    "While our initial trading strategy has shown promising results, the fact that the strategy is winning and outperforming the benchmark by trading all signals within a given quantile provides us with a solid foundation to build upon. This sets up a classic classification problem where our goal is to refine our strategy further. Our benchmark for success is to deliver better results than simply trading all signals within the selected quantiles.\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Enhance the dataset with additional features focusing primarily on momentum based features on the equity curve column.\n",
    "\n",
    "2. **Classification Algorithms**:\n",
    "   - Use machine learning algorithms to classify signals more effectively.\n",
    "   - Evaluate different models (e.g., logistic regression, decision trees, random forests and SVM) to identify the best-performing classifier.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - Assess the performance of the optimized strategy using metrics such as accuracy, precision, recall, F1-score from the confusion matrix.\n",
    "\n",
    "4. **Testing Model Significance**:\n",
    "   - We only want to incorporate the model into our trading strategy if the results we are encountering are statistically signficant\n",
    "\n",
    "### Approach\n",
    "\n",
    "We will begin by enhancing our dataset through feature engineering. Next, we will train and evaluate various classification models to predict the trading signals. We will then determine by looking at the confusion matrix output, if the model improvement is statistically significant, and worth adding to our strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineer\n",
    "\n",
    "Here we are creating the features to feed to the ML classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    signal  equity_curve  equity_returns     ema_3     ema_5  \\\n",
      "19          20  0.005743      0.362728        0.000544  0.363872  0.363505   \n",
      "20          21 -0.005063      0.362925        0.001600  0.363399  0.363312   \n",
      "21          22 -0.008720      0.363506       -0.012017  0.363452  0.363376   \n",
      "22          23 -0.003324      0.359138        0.022483  0.361295  0.361964   \n",
      "23          24 -0.006281      0.367212        0.003951  0.364254  0.363713   \n",
      "\n",
      "       ema_7    ema_10    ema_20  ema_3_7_crossover  ...  ema_10_20_crossover  \\\n",
      "19  0.363145  0.362914  0.363529                  1  ...                    0   \n",
      "20  0.363090  0.362916  0.363472                  1  ...                    0   \n",
      "21  0.363194  0.363023  0.363475                  1  ...                    0   \n",
      "22  0.362180  0.362317  0.363062                  0  ...                    0   \n",
      "23  0.363438  0.363207  0.363457                  1  ...                    0   \n",
      "\n",
      "       sma_3     sma_5     sma_7    sma_10    sma_20  sma_3_7_crossover  \\\n",
      "19  0.365060  0.363321  0.362221  0.362065  0.362813                  1   \n",
      "20  0.363988  0.364322  0.362805  0.362330  0.362463                  1   \n",
      "21  0.363053  0.364322  0.363291  0.362500  0.362327                  0   \n",
      "22  0.361856  0.362922  0.363465  0.362112  0.361844                  0   \n",
      "23  0.363285  0.363102  0.363995  0.362949  0.361842                  0   \n",
      "\n",
      "    sma_5_10_crossover  sma_10_20_crossover  sma_5_20_crossover  \n",
      "19                   1                    0                   1  \n",
      "20                   1                    0                   1  \n",
      "21                   1                    1                   1  \n",
      "22                   1                    1                   1  \n",
      "23                   1                    1                   1  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"../data/processed/processed_data.csv\")\n",
    "\n",
    "data['ema_3'] = data['equity_curve'].ewm(span=3, adjust=False).mean()\n",
    "data['ema_5'] = data['equity_curve'].ewm(span=5, adjust=False).mean()\n",
    "data['ema_7'] = data['equity_curve'].ewm(span=7, adjust=False).mean()\n",
    "data['ema_10'] = data['equity_curve'].ewm(span=10, adjust=False).mean()\n",
    "data['ema_20'] = data['equity_curve'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "data['ema_3_7_crossover'] = (data['ema_3'] > data['ema_7']).astype(int)\n",
    "data['ema_5_10_crossover'] = (data['ema_5'] > data['ema_10']).astype(int)\n",
    "data['ema_5_20_crossover'] = (data['ema_5'] > data['ema_20']).astype(int)\n",
    "data['ema_10_20_crossover'] = (data['ema_10'] > data['ema_20']).astype(int)\n",
    "\n",
    "data['sma_3'] = data['equity_curve'].rolling(window=3).mean()\n",
    "data['sma_5'] = data['equity_curve'].rolling(window=5).mean()\n",
    "data['sma_7'] = data['equity_curve'].rolling(window=7).mean()\n",
    "data['sma_10'] = data['equity_curve'].rolling(window=10).mean()\n",
    "data['sma_20'] = data['equity_curve'].rolling(window=20).mean()\n",
    "\n",
    "data['sma_3_7_crossover'] = (data['sma_3'] > data['sma_7']).astype(int)\n",
    "data['sma_5_10_crossover'] = (data['sma_5'] > data['sma_10']).astype(int)\n",
    "data['sma_10_20_crossover'] = (data['sma_10'] > data['sma_20']).astype(int)\n",
    "data['sma_5_20_crossover'] = (data['sma_5'] > data['sma_20']).astype(int)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data.to_csv(\"../data/processed/feature_engineered_data.csv\", index=False)\n",
    "\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    signal  equity_curve  equity_returns     ema_3     ema_5  \\\n",
      "19          20  0.005743      0.362728        0.000544  0.363872  0.363505   \n",
      "20          21 -0.005063      0.362925        0.001600  0.363399  0.363312   \n",
      "21          22 -0.008720      0.363506       -0.012017  0.363452  0.363376   \n",
      "22          23 -0.003324      0.359138        0.022483  0.361295  0.361964   \n",
      "23          24 -0.006281      0.367212        0.003951  0.364254  0.363713   \n",
      "\n",
      "       ema_7    ema_10    ema_20  ema_3_7_crossover  ...     sma_7    sma_10  \\\n",
      "19  0.363145  0.362914  0.363529                  1  ...  0.362221  0.362065   \n",
      "20  0.363090  0.362916  0.363472                  1  ...  0.362805  0.362330   \n",
      "21  0.363194  0.363023  0.363475                  1  ...  0.363291  0.362500   \n",
      "22  0.362180  0.362317  0.363062                  0  ...  0.363465  0.362112   \n",
      "23  0.363438  0.363207  0.363457                  1  ...  0.363995  0.362949   \n",
      "\n",
      "      sma_20  sma_3_7_crossover  sma_5_10_crossover  sma_10_20_crossover  \\\n",
      "19  0.362813                  1                   1                    0   \n",
      "20  0.362463                  1                   1                    0   \n",
      "21  0.362327                  0                   1                    1   \n",
      "22  0.361844                  0                   1                    1   \n",
      "23  0.361842                  0                   1                    1   \n",
      "\n",
      "    sma_5_20_crossover      macd  macd_signal  macd_hist  \n",
      "19                   1  0.000000     0.000000   0.000000  \n",
      "20                   1  0.000016     0.000003   0.000013  \n",
      "21                   1  0.000074     0.000017   0.000057  \n",
      "22                   1 -0.000229    -0.000032  -0.000197  \n",
      "23                   1  0.000180     0.000010   0.000169  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n",
    "    data['ema_short'] = data['equity_curve'].ewm(span=short_window, adjust=False).mean()\n",
    "    data['ema_long'] = data['equity_curve'].ewm(span=long_window, adjust=False).mean()\n",
    "    data['macd'] = data['ema_short'] - data['ema_long']\n",
    "    data['macd_signal'] = data['macd'].ewm(span=signal_window, adjust=False).mean()\n",
    "    data['macd_hist'] = data['macd'] - data['macd_signal']\n",
    "\n",
    "    return data\n",
    "\n",
    "data = calculate_macd(data)\n",
    "data = data.drop(columns=['ema_short', 'ema_long'])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    signal  equity_curve  equity_returns     ema_3     ema_5  \\\n",
      "20          21 -0.005063      0.362925        0.001600  0.363399  0.363312   \n",
      "21          22 -0.008720      0.363506       -0.012017  0.363452  0.363376   \n",
      "22          23 -0.003324      0.359138        0.022483  0.361295  0.361964   \n",
      "23          24 -0.006281      0.367212        0.003951  0.364254  0.363713   \n",
      "24          25 -0.004420      0.368663        0.017068  0.366458  0.365363   \n",
      "\n",
      "       ema_7    ema_10    ema_20  ema_3_7_crossover  ...    sma_10    sma_20  \\\n",
      "20  0.363090  0.362916  0.363472                  1  ...  0.362330  0.362463   \n",
      "21  0.363194  0.363023  0.363475                  1  ...  0.362500  0.362327   \n",
      "22  0.362180  0.362317  0.363062                  0  ...  0.362112  0.361844   \n",
      "23  0.363438  0.363207  0.363457                  1  ...  0.362949  0.361842   \n",
      "24  0.364744  0.364199  0.363953                  1  ...  0.363805  0.362201   \n",
      "\n",
      "    sma_3_7_crossover  sma_5_10_crossover  sma_10_20_crossover  \\\n",
      "20                  1                   1                    0   \n",
      "21                  0                   1                    1   \n",
      "22                  0                   1                    1   \n",
      "23                  0                   1                    1   \n",
      "24                  1                   1                    1   \n",
      "\n",
      "    sma_5_20_crossover      macd  macd_signal  macd_hist      rsi_14  \n",
      "20                   1  0.000016     0.000003   0.000013  100.000000  \n",
      "21                   1  0.000074     0.000017   0.000057  100.000000  \n",
      "22                   1 -0.000229    -0.000032  -0.000197   15.122285  \n",
      "23                   1  0.000180     0.000010   0.000169   66.959958  \n",
      "24                   1  0.000614     0.000131   0.000483   70.227046  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "def compute_rsi(data, window):\n",
    "    delta = data['equity_curve'].diff(1)\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "data['rsi_14'] = compute_rsi(data, 14)\n",
    "data = data.dropna()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the trading strategy\n",
    "\n",
    "We need create our quantiles in the training dataset, and then add the quantile data to both the test and training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bins:\n",
      "[-inf, -0.0126427120969868, -0.0083365484598235, -0.0053055001645362, -0.0026362883712754, -0.0001847297271523, 0.0022688327178055, 0.0049028381548227, 0.0079980306604964, 0.0123438110295806, 0.0372032712589485, inf]\n",
      "Quantile Summary:\n",
      "   signal_quantile       min       max\n",
      "0                0 -0.041569 -0.012643\n",
      "1                1 -0.012641 -0.008337\n",
      "2                2 -0.008336 -0.005306\n",
      "3                3 -0.005305 -0.002636\n",
      "4                4 -0.002636 -0.000185\n",
      "5                5 -0.000184  0.002269\n",
      "6                6  0.002269  0.004903\n",
      "7                7  0.004907  0.007998\n",
      "8                8  0.007999  0.012344\n",
      "9                9  0.012344  0.037203\n",
      "Buy quantiles: [7, 8, 9]\n",
      "Sell quantiles: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, shuffle=False)\n",
    "\n",
    "train_data['signal_quantile'] = pd.qcut(train_data['signal'], 10, labels=False)\n",
    "quantile_summary = train_data.groupby('signal_quantile')['signal'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "bins = [-np.inf] + quantile_summary['max'].tolist() + [np.inf]\n",
    "print(\"Bins:\")\n",
    "print(bins)\n",
    "\n",
    "print(\"Quantile Summary:\")\n",
    "print(quantile_summary)\n",
    "\n",
    "positive_signals_std = train_data[train_data['signal'] > 0]['signal'].std()\n",
    "negative_signals_std = train_data[train_data['signal'] < 0]['signal'].std()\n",
    "\n",
    "buy_quantiles = quantile_summary[quantile_summary['max'] >= positive_signals_std]['signal_quantile'].tolist()\n",
    "sell_quantiles = quantile_summary[quantile_summary['min'] <= -negative_signals_std]['signal_quantile'].tolist()\n",
    "\n",
    "print(f\"Buy quantiles: {buy_quantiles}\")\n",
    "print(f\"Sell quantiles: {sell_quantiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Trading Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    signal  equity_curve  equity_returns     ema_3     ema_5  \\\n",
      "20          21 -0.005063      0.362925        0.001600  0.363399  0.363312   \n",
      "21          22 -0.008720      0.363506       -0.012017  0.363452  0.363376   \n",
      "22          23 -0.003324      0.359138        0.022483  0.361295  0.361964   \n",
      "23          24 -0.006281      0.367212        0.003951  0.364254  0.363713   \n",
      "24          25 -0.004420      0.368663        0.017068  0.366458  0.365363   \n",
      "\n",
      "       ema_7    ema_10    ema_20  ema_3_7_crossover  ...  sma_3_7_crossover  \\\n",
      "20  0.363090  0.362916  0.363472                  1  ...                  1   \n",
      "21  0.363194  0.363023  0.363475                  1  ...                  0   \n",
      "22  0.362180  0.362317  0.363062                  0  ...                  0   \n",
      "23  0.363438  0.363207  0.363457                  1  ...                  0   \n",
      "24  0.364744  0.364199  0.363953                  1  ...                  1   \n",
      "\n",
      "    sma_5_10_crossover  sma_10_20_crossover  sma_5_20_crossover      macd  \\\n",
      "20                   1                    0                   1  0.000016   \n",
      "21                   1                    1                   1  0.000074   \n",
      "22                   1                    1                   1 -0.000229   \n",
      "23                   1                    1                   1  0.000180   \n",
      "24                   1                    1                   1  0.000614   \n",
      "\n",
      "    macd_signal  macd_hist      rsi_14  signal_quantile  trade_signal  \n",
      "20     0.000003   0.000013  100.000000                3             0  \n",
      "21     0.000017   0.000057  100.000000                1            -1  \n",
      "22    -0.000032  -0.000197   15.122285                3             0  \n",
      "23     0.000010   0.000169   66.959958                2            -1  \n",
      "24     0.000131   0.000483   70.227046                3             0  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "       Unnamed: 0    signal  equity_curve  equity_returns     ema_3     ema_5  \\\n",
      "35003       35004  0.005905      0.778394       -0.012145  0.777618  0.775352   \n",
      "35004       35005 -0.007350      0.768940       -0.004804  0.773279  0.773215   \n",
      "35005       35006  0.006328      0.765246        0.000261  0.769262  0.770558   \n",
      "35006       35007  0.008281      0.765445       -0.004526  0.767354  0.768854   \n",
      "35007       35008 -0.024470      0.761981        0.002272  0.764668  0.766563   \n",
      "\n",
      "          ema_7    ema_10    ema_20  ema_3_7_crossover  ...  \\\n",
      "35003  0.772961  0.770344  0.767668                  1  ...   \n",
      "35004  0.771956  0.770089  0.767790                  1  ...   \n",
      "35005  0.770278  0.769208  0.767547                  0  ...   \n",
      "35006  0.769070  0.768524  0.767347                  0  ...   \n",
      "35007  0.767298  0.767335  0.766836                  0  ...   \n",
      "\n",
      "       sma_3_7_crossover  sma_5_10_crossover  sma_10_20_crossover  \\\n",
      "35003                  1                   1                    1   \n",
      "35004                  1                   1                    1   \n",
      "35005                  0                   1                    1   \n",
      "35006                  0                   1                    1   \n",
      "35007                  0                   0                    1   \n",
      "\n",
      "       sma_5_20_crossover      macd  macd_signal  macd_hist     rsi_14  \\\n",
      "35003                   1  0.001261    -0.002239   0.003500  61.085756   \n",
      "35004                   1  0.001145    -0.001562   0.002707  54.199160   \n",
      "35005                   1  0.000747    -0.001100   0.001847  50.547549   \n",
      "35006                   1  0.000443    -0.000792   0.001234  58.238483   \n",
      "35007                   1 -0.000078    -0.000649   0.000571  59.812990   \n",
      "\n",
      "       signal_quantile  trade_signal  \n",
      "35003                7             1  \n",
      "35004                2            -1  \n",
      "35005                7             1  \n",
      "35006                8             1  \n",
      "35007                0            -1  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "def assign_trading_signals(data, bins, buy_quantiles, sell_quantiles):\n",
    "    data['signal_quantile'] = pd.cut(data['signal'], bins=bins, labels=False, include_lowest=True)\n",
    "    \n",
    "    data['trade_signal'] = 0\n",
    "\n",
    "    data.loc[data['signal_quantile'].isin(buy_quantiles), 'trade_signal'] = 1\n",
    "    data.loc[data['signal_quantile'].isin(sell_quantiles), 'trade_signal'] = -1\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data_with_signals = assign_trading_signals(train_data, bins, buy_quantiles, sell_quantiles)\n",
    "test_data_with_signals = assign_trading_signals(test_data, bins, buy_quantiles, sell_quantiles)\n",
    "\n",
    "print(train_data.head(5))\n",
    "print(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the quantile to build model for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q4/qlhh3zds71n242024cd1plqr0000gn/T/ipykernel_75944/3389875699.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  positive_quartile_filtered_train_data['profitable_trade'] = (positive_quartile_filtered_train_data['equity_returns'] > 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "positive_quartile_filtered_train_data = train_data[train_data['signal_quantile'].isin([9])]\n",
    "positive_quartile_filtered_train_data['profitable_trade'] = (positive_quartile_filtered_train_data['equity_returns'] > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the features \n",
    "\n",
    "Here we set the features and target for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2799, 5)\n",
      "X_val shape: (700, 5)\n",
      "y_train shape: (2799,)\n",
      "y_val shape: (700,)\n"
     ]
    }
   ],
   "source": [
    "features = [ \n",
    "    'signal',\n",
    "    'macd',  \n",
    "    'rsi_14',\n",
    "    'ema_3_7_crossover',\n",
    "    'sma_5_10_crossover',\n",
    "]\n",
    "\n",
    "# features = [\n",
    "#     'equity_curve_lag_1',\n",
    "#     'equity_curve_lag_2',\n",
    "#     'equity_curve_lag_3',\n",
    "#     'equity_curve_ma_3',\n",
    "#     'equity_curve_ma_5',\n",
    "#     'equity_curve_ma_10',\n",
    "#     'sma_5',\n",
    "#     'sma_10',\n",
    "#     'sma_20',\n",
    "#     'sma_5_10_crossover',\n",
    "#     'sma_10_20_crossover',\n",
    "#     'sma_5_20_crossover'\n",
    "# ]\n",
    "\n",
    "target = 'profitable_trade'\n",
    "X = positive_quartile_filtered_train_data[features]\n",
    "y = positive_quartile_filtered_train_data[target]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Here we are able to run all the models, we also provided a print out of the feature coefficients for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Feature  Coefficient\n",
      "1                macd    -0.609289\n",
      "2              rsi_14    -0.352655\n",
      "3   ema_3_7_crossover    -0.137188\n",
      "4  sma_5_10_crossover    -0.036717\n",
      "0              signal    -0.019098\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_val_scaled)\n",
    "\n",
    "feature_names = X_train.columns\n",
    "coefficients = log_reg.coef_[0] \n",
    "\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "coeff_df['abs_coefficient'] = coeff_df['Coefficient'].abs()\n",
    "coeff_df = coeff_df.sort_values(by='abs_coefficient', ascending=False)\n",
    "print(coeff_df.drop(columns='abs_coefficient'))\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_val)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_val)\n",
    "\n",
    "svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.49      0.52       345\n",
      "           1       0.55      0.62      0.58       355\n",
      "\n",
      "    accuracy                           0.55       700\n",
      "   macro avg       0.55      0.55      0.55       700\n",
      "weighted avg       0.55      0.55      0.55       700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[169 176]\n",
      " [136 219]]\n",
      "Accuracy: 0.5542857142857143\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.48      0.49       345\n",
      "           1       0.51      0.52      0.52       355\n",
      "\n",
      "    accuracy                           0.50       700\n",
      "   macro avg       0.50      0.50      0.50       700\n",
      "weighted avg       0.50      0.50      0.50       700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[167 178]\n",
      " [169 186]]\n",
      "Accuracy: 0.5042857142857143\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.50       345\n",
      "           1       0.53      0.56      0.54       355\n",
      "\n",
      "    accuracy                           0.52       700\n",
      "   macro avg       0.52      0.52      0.52       700\n",
      "weighted avg       0.52      0.52      0.52       700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[168 177]\n",
      " [157 198]]\n",
      "Accuracy: 0.5228571428571429\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_log_reg))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_log_reg))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_log_reg))\n",
    "\n",
    "print(\"\\nDecision Tree Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_dt))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_dt))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_dt))\n",
    "\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.62      0.59       345\n",
      "           1       0.59      0.52      0.55       355\n",
      "\n",
      "    accuracy                           0.57       700\n",
      "   macro avg       0.57      0.57      0.57       700\n",
      "weighted avg       0.57      0.57      0.57       700\n",
      "\n",
      "Confusion Matrix:\n",
      " [[215 130]\n",
      " [170 185]]\n",
      "Accuracy: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_svm))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_svm))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistically significant results from SVM classification !!\n",
    "\n",
    "The results show that we have 400 correct decisions using this model. Whereas if we had just traded all of the opportunities we would have been correct only 355 times. \n",
    "\n",
    "We come up with this buy looking at the 185 true positives where we guessed that a trade was profitable, and then look at the 215 times where we didn't trade because we correctly guessed that a trade was unprofitable.\n",
    "\n",
    "SVM Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.56      0.62      0.59       345\n",
    "           1       0.59      0.52      0.55       355\n",
    "\n",
    "    accuracy                           0.57       700\n",
    "   macro avg       0.57      0.57      0.57       700\n",
    "weighted avg       0.57      0.57      0.57       700\n",
    "\n",
    "Confusion Matrix:\n",
    " [[215 130]\n",
    " [170 185]]\n",
    "Accuracy: 0.5714285714285714\n",
    "\n",
    "\n",
    "The following features:\n",
    "     - macd\n",
    "     - rsi_14\n",
    "     - ema_3_7_crossover\n",
    "     - sma_5_10_crossover\n",
    "     - signal\n",
    "\n",
    "\n",
    "**SVM Strategy**:\n",
    "\n",
    "Total Trades Executed: 315\n",
    "\n",
    "Winning Trades: 185\n",
    "\n",
    "Losing Trades: 130\n",
    "\n",
    "Net Winning Trades: 55\n",
    "\n",
    "**Baseline Strategy**:\n",
    "\n",
    "Total Trades Executed: 700\n",
    "\n",
    "Winning Trades: 355\n",
    "\n",
    "Losing Trades: 345\n",
    "\n",
    "Net Winning Trades: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use a z statistic and a p-value to test to determine if the values we are returing are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-statistic: 2.412810412987092\n",
      "P-value: 0.015830051339935088\n",
      "The difference is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "wins_group1 = 400\n",
    "wins_group2 = 355\n",
    "\n",
    "trials_group1 = 700\n",
    "trials_group2 = 700\n",
    "\n",
    "count = np.array([wins_group1, wins_group2])\n",
    "nobs = np.array([trials_group1, trials_group2])\n",
    "\n",
    "stat, pval = proportions_ztest(count, nobs)\n",
    "\n",
    "print(f\"Z-statistic: {stat}\")\n",
    "print(f\"P-value: {pval}\")\n",
    "\n",
    "if pval < 0.05:\n",
    "    print(\"The difference is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "We can see that the current model is statistically significant in improving our trading of the 9 quantile group. This could prove lucrative as this quantile has promise to be one of our most promising. While the model trades less, it does have a higher ratio of correct trades to incorrect trades. \n",
    "\n",
    "In terms of optimization, this could also lead us to increase the sizing our trading while using this model, because the model is 59% accurate in correctly guessing a 50/50 choice we could look at increasing the bet sizing whenever this model is making a decision. \n",
    "\n",
    "We could also further improve the model by having it increase sizing based on the confidence statistic of the SVM model, whereas if the SVM classification model has a high confidence in its choice, we significantly increase our bet size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
